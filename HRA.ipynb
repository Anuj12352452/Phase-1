{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd # for data wrangling purpose\nimport numpy as np # Basic computation library\nimport seaborn as sns # For Visualization \nimport matplotlib.pyplot as plt # ploting package\n%matplotlib inline\nimport warnings # Filtering warnings\nwarnings.filterwarnings('ignore')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Importing IBM HR dataset Csv file using pandas",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR=pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print('No of Rows:',HR.shape[0])\nprint('No of Columns:',HR.shape[1])\npd.set_option('display.max_columns', None) # This will enable us to see truncated columns\nHR.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR.columns",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# As we have 35 Columns Lets sort Columns by their datatype\nHR.columns.to_series().groupby(HR.dtypes).groups",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Statistical Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "HR.duplicated().sum()  # This will check the duplicate data for all columns.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Missing value check",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing_values = HR.isnull().sum().sort_values(ascending = False)\npercentage_missing_values =(missing_values/len(HR))*100\nprint(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Visualizing the statistics of the columns using heatmap.\nplt.figure(figsize=(28,10))\nsns.heatmap(HR.describe(),linewidths = 0.1,fmt='0.1f',annot = True,cmap='PiYG')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR.describe().T.round(3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR['Attrition'].value_counts()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "labels = 'Yes','No',\nfig, ax = plt.subplots()\nax.pie(HR['Attrition'].value_counts(),labels = labels,radius =2,autopct = '%2.2f%%',explode=[0.1,0.2], shadow=True,)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Category=['Attrition', 'BusinessTravel', 'Department', 'EducationField',\n          'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime' ]\nfor i in Category:\n    print(i)\n    print(HR[i].value_counts())\n    print(\"=\"*100)\n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "sns.set_palette('gist_rainbow_r')\nplt.figure(figsize=(20,20), facecolor='white')\nplotnumber =1\nCategory=['Attrition', 'BusinessTravel', 'Department', 'EducationField',\n          'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime' ]\nfor i in Category:\n    if plotnumber <=9:\n        ax = plt.subplot(3,3,plotnumber)\n        sns.countplot(HR[i])\n        plt.xlabel(i,fontsize=20)\n        plt.xticks(rotation=30)\n    plotnumber+=1\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Ordinal=['Education','EnvironmentSatisfaction', 'JobInvolvement','JobSatisfaction',\n          'RelationshipSatisfaction', 'PerformanceRating', 'WorkLifeBalance' ]\nfor i in Ordinal:\n    print(i)\n    print(HR[i].value_counts())\n    print(\"=\"*100)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "sns.set_palette('hsv')\nplt.figure(figsize=(20,20), facecolor='white')\nplotnumber =1\nOrdinal=['Education','EnvironmentSatisfaction', 'JobInvolvement','JobSatisfaction',\n          'RelationshipSatisfaction', 'PerformanceRating', 'WorkLifeBalance' ]\nfor i in Ordinal:\n    if plotnumber <=9:\n        ax = plt.subplot(3,3,plotnumber)\n        sns.countplot(HR[i])\n        plt.xlabel(i,fontsize=20)\n        plt.xticks(rotation=30)\n    plotnumber+=1\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "labels='Bachelor','Master','College','Below College','Doctor'\nfig, ax = plt.subplots()\nax.pie(HR['Education'].value_counts(),labels = labels,radius =2,autopct = '%3.2f%%',explode=[0.1,0.1,0.15,0.2,0.3], shadow=True,)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR['Department'].value_counts()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "labels ='Research & Development','Sales','Human Resources'\nfig,ax= plt.subplots()\nax.pie(HR['Department'].value_counts(),labels=labels, radius=2,autopct= '%3.2f%%',explode=[0.1,0.15,0.2],shadow=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "pd.crosstab([HR.Education],[HR.Department], margins=True).style.background_gradient(cmap='summer_r')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "pd.crosstab([HR.Education],[HR.Department,HR.Attrition], margins=True).style.background_gradient(cmap='summer_r')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR['EducationField'].value_counts()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "labels ='Life Sciences','Medical','Marketing','Technical Degree','Other','Human Resources'\nfig,ax= plt.subplots()\nax.pie(HR['EducationField'].value_counts(),labels=labels, radius=2,autopct= '%3.2f%%',explode=[0.1,0.1,0.125,0.15,0.15,0.175],shadow=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Let check distribution of education Vs education Field\npd.crosstab([HR.Education],[HR.EducationField], margins=True).style.background_gradient(cmap='summer_r')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Let check distribution of department Vs education Field\npd.crosstab([HR.Department],[HR.EducationField], margins=True).style.background_gradient(cmap='summer_r')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(15,7))\nsns.countplot(HR['JobRole'])\nplt.xticks(rotation=45)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "pd.crosstab([HR.JobRole],[HR.Department], margins=True).style.background_gradient(cmap='gist_rainbow_r')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,10))\ndata=pd.crosstab(HR['JobRole'], HR['Attrition'])\ndata.div(data.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, \n                    color=['green', 'red'],figsize=(12,8))\nplt.title('Attrition as per Job Role', fontsize=20)\nplt.xlabel('Job Role',fontsize=15)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "pd.crosstab([HR.JobRole,HR.Department],[HR.Attritionmargins=True).style.background_gradient(cmap='gist_rainbow_r')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Grouping Numeric Features\nNumeric=['Age', 'DailyRate', 'DistanceFromHome',  \n 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', \n 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', \n 'RelationshipSatisfaction','StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Violinplot of Numeric Variables",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Grouping Numeric Features\nNumeric_int=['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate','MonthlyIncome', 'MonthlyRate', \n             'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears','TrainingTimesLastYear',\n             'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "sns.set_palette('spring')\nplt.figure(figsize=(20,50), facecolor='white')\nplotnumber =1\n\nfor i in Numeric_int:\n    if plotnumber <=25:\n        ax = plt.subplot(9,3,plotnumber)\n        sns.violinplot(HR[i])\n        plt.xlabel(i,fontsize=20)\n        plt.xticks(rotation=30)\n    plotnumber+=1\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Age Vs Attrition",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.subplots(figsize=(12,6))\nsns.countplot(HR['Age'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "sns.set_palette('hsv')\nplt.subplots(figsize=(18,8))\nsns.countplot(x='Age', hue='Attrition', data=HR)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(15,8))\nsns.barplot(HR['TotalWorkingYears'],HR['MonthlyIncome'])\nplt.xlabel('Total Working Years',fontsize=20)\nplt.ylabel('Monthly Income',fontsize=20)\nplt.title(\" Total Working Years vs Monthly Income\", fontsize=20)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(8,6))\nsns.barplot(x='Attrition',y='MonthlyIncome',data=HR)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(8,6))\nsns.barplot(x='Attrition',y='YearsSinceLastPromotion',data=HR)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR=pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Encoding categorical data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Using Label Encoder on target variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nHR[\"Attrition\"] = le.fit_transform(HR[\"Attrition\"])\nHR.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Droping unnecessary columns\nHR.drop([\"EmployeeCount\", \"EmployeeNumber\", \"Over18\", \"StandardHours\"], axis=1, inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Ordinal Encoding for ordinal variables\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\ndef ordinal_encode(HR, column):\n    HR[column] = oe.fit_transform(HR[column])\n    return HR\n\noe_col = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\nHR=ordinal_encode(HR, oe_col)\nHR.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Outliers Detection and Removal",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(20,30),facecolor='white')\nplotnumber=1\n\nfor column in Numeric:\n    if plotnumber<=28:\n        ax=plt.subplot(7,4,plotnumber)\n        sns.boxplot(HR[column],color='g')\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from scipy.stats import zscore\nz = np.abs(zscore(HR))\nthreshold = 3\nHR1 = HR[(z<3).all(axis = 1)]\n\nprint (\"Shape of the dataframe before removing outliers: \", HR.shape)\nprint (\"Shape of the dataframe after removing outliers: \", HR1.shape)\nprint (\"Percentage of data loss post outlier removal: \", (HR.shape[0]-HR1.shape[0])/HR.shape[0]*100)\n\nHR=HR1.copy() # reassigning the changed dataframe name to our original dataframe name",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((1470-1387)/1470)*100,'%')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Feature selection and Engineering",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "HR.skew()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Splitting data in target and dependent feature\nX = HR.drop(['Attrition'], axis =1)\nY = HR['Attrition']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import power_transform\nHR = power_transform(x)\nHR = pd.DataFrame(HR, columns=x.columns)\nHR.skew()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "HR.corr()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "upper_triangle = np.triu(HR.corr())\nplt.figure(figsize=(25,15))\nsns.heatmap(HR.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n            annot_kws={'size':10}, cmap=\"gist_stern\", mask=upper_triangle)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize = (18,6))\nHR1.corr()['Attrition'].drop(['Attrition']).plot(kind='bar',color = 'c')\nplt.xlabel('Features',fontsize=15)\nplt.ylabel('Attrition',fontsize=15)\nplt.title('Correlation of features with Target Variable Attrition',fontsize = 18)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif= pd.DataFrame()\nvif['VIF']= [variance_inflation_factor(HR.values,i) for i in range(HR.shape[1])]\nvif['Features']= HR.columns\nvif",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from imblearn.over_sampling import SMOTE",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Oversampleing using SMOTE Techniques\noversample = SMOTE()\nX, Y = oversample.fit_resample(X, Y)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Y.value_counts()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Standard Scaling",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nX_scale = scaler.fit_transform(X)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\npca = PCA()\n#plot the graph to find the principal components\nx_pca = pca.fit_transform(X_scale)\nplt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.xlabel('Number of Components')\nplt.ylabel('Variance %')\nplt.title('Explained variance Ratio')\nplt.grid()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "pca_new = PCA(n_components=21)\nx_new = pca_new.fit_transform(X_scale)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "principle_x=pd.DataFrame(x_new,columns=np.arange(21))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Machine Learning Model Building",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=42, test_size=.33)\nprint('Training feature matrix size:',X_train.shape)\nprint('Training target vector size:',Y_train.shape)\nprint('Test feature matrix size:',X_test.shape)\nprint('Test target vector size:',Y_test.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Finding best Random state",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\nmaxAccu=0\nmaxRS=0\nfor i in range(1,250):\n    X_train,X_test,Y_train,Y_test = train_test_split(principle_x,Y,test_size = 0.33, random_state=i)\n    log_reg=LogisticRegression()\n    log_reg.fit(X_train,Y_train)\n    y_pred=log_reg.predict(X_test)\n    acc=accuracy_score(Y_test,y_pred)\n    if acc>maxAccu:\n        maxAccu=acc\n        maxRS=i\nprint('Best accuracy is', maxAccu ,'on Random_state', maxRS)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=242, test_size=.33)\nlog_reg=LogisticRegression()\nlog_reg.fit(X_train,Y_train)\ny_pred=log_reg.predict(X_test)\nprint('\\033[1m'+'Logistics Regression Evaluation'+'\\033[0m')\nprint('\\n')\nprint('\\033[1m'+'Accuracy Score of Logistics Regression :'+'\\033[0m', accuracy_score(Y_test, y_pred))\nprint('\\n')\nprint('\\033[1m'+'Confusion matrix of Logistics Regression :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\nprint('\\n')\nprint('\\033[1m'+'classification Report of Logistics Regression'+'\\033[0m \\n',classification_report(Y_test, y_pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Finding Optimal value of n_neighbors for KNN",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn import neighbors\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nrmse_val = [] #to store rmse values for different k\nfor K in range(30):\n    K = K+1\n    model = neighbors.KNeighborsClassifier(n_neighbors = K)\n\n    model.fit(X_train,Y_train)  #fit the model\n    y_pred=model.predict(X_test) #make prediction on test set\n    error = sqrt(mean_squared_error(Y_test,y_pred)) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#plotting the rmse values against k values -\nplt.figure(figsize = (8,6))\nplt.plot(range(30), rmse_val, color='blue', linestyle='dashed', marker='o', markerfacecolor='green', markersize=10)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Applying other classification algorithm",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model=[\n        SVC(),\n        GaussianNB(),\n        DecisionTreeClassifier(),\n        KNeighborsClassifier(n_neighbors = 22),\n        RandomForestClassifier(),\n        AdaBoostClassifier(),\n        GradientBoostingClassifier(),\n        BaggingClassifier()]\n\nfor m in model:\n    m.fit(X_train,Y_train)\n    y_pred=m.predict(X_test)\n    print('\\033[1m'+'Classification ML Algorithm Evaluation Matrix',m,'is' +'\\033[0m')\n    print('\\n')\n    print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))\n    print('\\n')\n    print('\\033[1m'+'Confusion matrix :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n    print('\\n')\n    print('\\033[1m'+'Classification Report :'+'\\033[0m \\n',classification_report(Y_test, y_pred))\n    print('\\n')\n    print('============================================================================================================')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "CrossValidation :",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\nmodel=[LogisticRegression(),\n        SVC(),\n        GaussianNB(),\n        DecisionTreeClassifier(),\n        KNeighborsClassifier(n_neighbors = 12),\n        RandomForestClassifier(),\n        AdaBoostClassifier(),\n        GradientBoostingClassifier(),\n        BaggingClassifier()]\n\nfor m in model:\n    score = cross_val_score(m, X, Y, cv =5)\n    print('\\n')\n    print('\\033[1m'+'Cross Validation Score', m, ':'+'\\033[0m\\n')\n    print(\"Score :\" ,score)\n    print(\"Mean Score :\",score.mean())\n    print(\"Std deviation :\",score.std())\n    print('\\n')\n    print('============================================================================================================')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Hyper Parameter Tuning : GridSearchCV",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "parameter = {  'bootstrap': [True], 'max_depth': [5, 10,20,40,50, None], \n              'max_features': ['auto', 'log2'], \n              'criterion':['gini','entropy'],\n              'n_estimators': [5, 10, 15 ,25,50,100]}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "GCV = GridSearchCV(RandomForestClassifier(),parameter,cv=5,n_jobs = -1,verbose=3)\nGCV.fit(X_train,Y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "GCV.best_params_",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Final Model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Final_mod = RandomForestClassifier(bootstrap=True,criterion='entropy',n_estimators= 25, max_depth=20 ,max_features='log2')\nFinal_mod.fit(X_train,Y_train)\ny_pred=Final_mod.predict(X_test)\nprint('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\ny_pred_prob = Final_mod.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(Y_test,y_pred_prob)\nplt.plot([0,1],[0,1], 'k--')\nplt.plot(fpr, tpr, label='Random Forest Classifier')\nplt.xlabel('False postive rate')\nplt.ylabel('True postive rate')\nplt.show()\nauc_score = roc_auc_score(Y_test, Final_mod.predict(X_test))\nprint('\\033[1m'+'Auc Score :'+'\\033[0m\\n',auc_score)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Saving model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import joblib\njoblib.dump(Final_mod,'IBM_HR_Analytics_Final",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}